{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling tuning in XGBoost\n",
    "\n",
    "**Miao Cai**\n",
    "\n",
    "When performing parameter searches, we will use a dictionary that we typically call a parameter grid, because it will contain ranges of values over which we will search to find an optimal configuration.\n",
    "\n",
    "Important parameters in XGBoost:\n",
    "\n",
    "- \"objective\": \"reg:linear\"\n",
    "- \"colsample_bytree\": 0.3\n",
    "- \"learning rate\": 0.1\n",
    "- \"max_depth\": 5\n",
    "\n",
    "Instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within `xgb.cv()`. This is done using a technique called **early stopping**.\n",
    "\n",
    "**Early stopping** works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "\n",
    "Here you will use the `early_stopping_rounds` parameter in `xgb.cv()` with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when `num_boosting_rounds` is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost hyperparameters\n",
    "\n",
    "**Common tree tunable parameters**\n",
    "\n",
    "- learning rate (`eta`): how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost model with a high learning rate. A high learning rate will penalize feature weights more strongly, causing much stronger regularization.\n",
    "- gamma: min loss reduction to create new tree split\n",
    "- lambda: L2 reg on leaf weights\n",
    "- alpha: L1 reg on leaf weights\n",
    "- max_depth: max depth per tree\n",
    "- subsample: % sample used per tree (between 0 and 1). If the value is low, the fraction of your training data used would per boosting round would be low, and you may run into underfitting problems. A value that is very high can lead to overfitting as well.\n",
    "- colsample_bytree: the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. A large value means that almost all features can be used to build a tree during a given boosting, whereas a small value means that the fraction of features that can be selected from is very small. In general, smaller colsample_bytree values can be thought of as providing additional regularization to the model, whereas using all columns may in certain cases overfit a trained model.\n",
    "\n",
    "gamma, lambda, and alpha will have an effect on how strongly regularized the trained model will be.\n",
    "\n",
    "**Linear tunable parameters**\n",
    "\n",
    "The number of tunable parameters is significantly smaller.\n",
    "\n",
    "- lambda: L2 reg on weights\n",
    "- alpha: L1 reg on weights\n",
    "- lambda_bias: L2 reg term on bias\n",
    "\n",
    "You can also tune the number of estimators used for both base model types, which is hte number of boosting rounds (either the number of trees you build or the number of linear base learners you construct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "#  params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "#  eta_vals = [0.001, 0.01, 0.1]\n",
    "#  best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "#  for curr_val in eta_vals:\n",
    "\n",
    "#      params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "#      cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, \n",
    "#          early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "#      best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "#  print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search and random search\n",
    "\n",
    "**Grid search**: a method of exhausively searching through a collection of possible parameter values.\n",
    "\n",
    "- The number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "- Pick final model hyperparameter values that give best cross-validated evaluation metric value\n",
    "\n",
    "**Random search**: simply involves drawing a random combination of possible hyperparameter values from the range of allowable hyperparameters a set number of times.\n",
    "\n",
    "- Create a (possible infinite) range of hyperparameter values per hyperparameter that you would like to search over.\n",
    "- Set the number of iterations you would like for the random search to continue\n",
    "- During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters.\n",
    "- After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "#  gbm_param_grid = {\n",
    "#     'colsample_bytree': [0.3, 0.7],\n",
    "#     'n_estimators': [50],\n",
    "#     'max_depth': [2, 5]\n",
    "#  }\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "#  gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "#  grid_mse = GridSearchCV(estimator=gbm, \n",
    "#  param_grid=gbm_param_grid,\n",
    "#  scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "#  grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "#  print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "#  print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The limites of grid search and random search\n",
    "\n",
    "**Grid search**:\n",
    "\n",
    "- Number of models you must build with every additional new parameter grows very quickly\n",
    "\n",
    "**Random search**\n",
    "\n",
    "- Parameter space to explore can be massive\n",
    "- Randomly jumping throughout the space looking for a \"best\" result becomes a waiting game.\n",
    "\n",
    "You can always increase the number of iterations you want the random search to run, but then finding an optimal configuration becomes a combination of waiting randomly finding a good set of hyperparameters.\n",
    "\n",
    "The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36ws",
   "language": "python",
   "name": "p36ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
