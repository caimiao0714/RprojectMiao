{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost to perform regression\n",
    "\n",
    "**Miao Cai**\n",
    "\n",
    "This is based on the datacamp online tutorial *Extreme Gradient Boosting with XGBoost*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common regression metrics\n",
    "\n",
    "- Root mean squared error (RMSE)\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "**RMSE** tends to punish larger differences between predicted and actual values much more than smaller ones. \n",
    "**MSE** tends to sum absolute differences across all of the samples we build our model on. \n",
    "Although MAE is not affected by large differences as much as RMSE, it lacks some nice matchematical properties that make it much less frequently used as an evaluation metric.\n",
    "\n",
    "**Common regression algorithms**\n",
    "\n",
    "- Linear regression\n",
    "- Decision trees\n",
    "\n",
    "Decision trees can be both used as regression or classification tools, whihc is an important property that makes them prime candidates to be building blocks for XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective (loss) functions and base learners\n",
    "\n",
    "An objective function (loss function) quantifies how far off a prediction is from the actual results for a given data point. It measures the difference between estimated and true value for some collection of data. The goal of building a machine learning model is to find the model that yields the minimum value of the loss function.\n",
    "\n",
    "**Common loss functions and XGBoost**\n",
    "\n",
    "- Loss function names in xgboost:\n",
    "    + reg:linear - use for regression problems\n",
    "    + reg:logistic - use for classification problems when you want just decision, not probability\n",
    "    + binary:logistic - use when you want probability rather than just decision\n",
    "\n",
    "**Base learners and why we need them**\n",
    "\n",
    "- XGBoost involves creating a meta-model that is composed of **many individual models** that **combine** to give a final prediction.\n",
    "- Individual models = base learners\n",
    "- We want base learners that when combined create final prediction that is **non-linear**\n",
    "- Each base learner should be good at distinguishing or predicting different part of the dataset\n",
    "\n",
    "The goal of XGBoost is to have base learners that is slightly better than random guessing on certain subsets of training examples and uniformly bad at the remainder, so that when all of the predictions are combined, the uniformly bad predictions cancel out and those slightly better than chance combine into a single very good prediction.\n",
    "\n",
    "By default, XGBoost use trees as base learners, which is `booster=\"gbtree\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear learners**\n",
    "\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as `xgb.train()`.\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used `xgb.cv()`). The key-value pair that defines the booster type (base model) you need is `\"booster\":\"gblinear\"`. Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "#  DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "#  DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#  params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#  xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#  preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "#  rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "#  print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating model quality\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#  params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#  cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#  print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "#  print((cv_results[\"params\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in XGBoost\n",
    "\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    + gamma - minimum loss reduction allowed for a split to occur\n",
    "    + alpha - l1 regularization on **leaf weights** (not on feature weights), larger values mean more regularization, which causes many leaf weights in the base learner to go to 0.\n",
    "    + lambda - l2 regularization on leaf weights, l2 regularization is a much smoother penalty than l1 and causes leaf weights to smoothly decrease, instead of strong sparsity constrains on the leaf weights as l1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base learners in XGBoost\n",
    "\n",
    "- Linear Base Learners:\n",
    "    + Sum of linear terms\n",
    "    + Boosted model is weighted sum of linear models\n",
    "    + rarely used (you do not get any non-linear combinations of features in the final model, so you will get identical performance from a regularized linear model).\n",
    "- Tree based learner:\n",
    "    + Decision tree\n",
    "    + Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    + Almost exclusively used in XGBoost\n",
    "\n",
    "In Python 3, `zip` creates a generator, or an object that doesn't have to be completely instantiated at runtime.\n",
    "\n",
    "- `pd.DataFrame(list(zip(list1, list2)), columns=[\"list1\", \"list2\"])`\n",
    "- `zip([1, 2, 3], [\"a\", \"b\", \"c\"]) = [1, \"a\"], [2, \"b\"], [3, \"c\"]`\n",
    "- `generators` need to be completely instantiated before they can be used in `DataFrame` objects.\n",
    "\n",
    "`list()` instantiates the full generator and passing that into the `DataFrame` converts the whole expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "#  reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "#  params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "#  rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "# for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "#     params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "#     cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, \n",
    "#          num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "#     rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "#  print(\"Best rmse as a function of l2:\")\n",
    "#  print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing individual XGBoost trees\n",
    "\n",
    "XGBoost has a `plot_tree()` function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the `plot_tree()` function along with the number of trees you want to plot using the `num_trees` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#  params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#  xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "#  xgb.plot_tree(xg_reg, num_trees=0, rankdir=\"LR\")\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing feature importances\n",
    "\n",
    "What features are most important in my dataset?\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a `plot_importance()` function that allows you to do exactly this, and you'll get a chance to use it in this exercise!\n",
    "\n",
    "`xgb.plot_importance(xg_reg)`\n",
    "`plt.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36ws",
   "language": "python",
   "name": "p36ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
