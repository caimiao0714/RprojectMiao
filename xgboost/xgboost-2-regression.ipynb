{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost to perform regression\n",
    "\n",
    "**Miao Cai**\n",
    "\n",
    "This is based on the datacamp online tutorial *Extreme Gradient Boosting with XGBoost*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common regression metrics\n",
    "\n",
    "- Root mean squared error (RMSE)\n",
    "- Mean absolute error (MAE)\n",
    "\n",
    "**RMSE** tends to punish larger differences between predicted and actual values much more than smaller ones. \n",
    "**MSE** tends to sum absolute differences across all of the samples we build our model on. \n",
    "Although MAE is not affected by large differences as much as RMSE, it lacks some nice matchematical properties that make it much less frequently used as an evaluation metric.\n",
    "\n",
    "**Common regression algorithms**\n",
    "\n",
    "- Linear regression\n",
    "- Decision trees\n",
    "\n",
    "Decision trees can be both used as regression or classification tools, whihc is an important property that makes them prime candidates to be building blocks for XGBoost models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective (loss) functions and base learners\n",
    "\n",
    "An objective function (loss function) quantifies how far off a prediction is from the actual results for a given data point. It measures the difference between estimated and true value for some collection of data. The goal of building a machine learning model is to find the model that yields the minimum value of the loss function.\n",
    "\n",
    "**Common loss functions and XGBoost**\n",
    "\n",
    "- Loss function names in xgboost:\n",
    "    + reg:linear - use for regression problems\n",
    "    + reg:logistic - use for classification problems when you want just decision, not probability\n",
    "    + binary:logistic - use when you want probability rather than just decision\n",
    "\n",
    "**Base learners and why we need them**\n",
    "\n",
    "- XGBoost involves creating a meta-model that is composed of **many individual models** that **combine** to give a final prediction.\n",
    "- Individual models = base learners\n",
    "- We want base learners that when combined create final prediction that is **non-linear**\n",
    "- Each base learner should be good at distinguishing or predicting different part of the dataset\n",
    "\n",
    "The goal of XGBoost is to have base learners that is slightly better than random guessing on certain subsets of training examples and uniformly bad at the remainder, so that when all of the predictions are combined, the uniformly bad predictions cancel out and those slightly better than chance combine into a single very good prediction.\n",
    "\n",
    "By default, XGBoost use trees as base learners, which is `booster=\"gbtree\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear learners**\n",
    "\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as `xgb.train()`.\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used `xgb.cv()`). The key-value pair that defines the booster type (base model) you need is `\"booster\":\"gblinear\"`. Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "#  DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "#  DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#  params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "#  xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "#  preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "#  rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "#  print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating model quality\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "#  housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "#  params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "#  cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "#  print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "#  print((cv_results[\"params\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization in XGBoost\n",
    "\n",
    "- Regularization is a control on model complexity\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    + gamma - minimum loss reduction allowed for a split to occur\n",
    "    + alpha - l1 regularization on **leaf weights** (not on feature weights), larger values mean more regularization, which causes many leaf weights in the base learner to go to 0.\n",
    "    + lambda - l2 regularization on leaf weights, l2 regularization is a much smoother penalty than l1 and causes leaf weights to smoothly decrease, instead of strong sparsity constrains on the leaf weights as l1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base learners in XGBoost\n",
    "\n",
    "- Linear Base Learners:\n",
    "        + Sum of linear terms\n",
    "        + Boosted model is weighted sum of linear models\n",
    "        (you do not get any non-linear combinations of )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36ws",
   "language": "python",
   "name": "p36ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
