---
title: "Instrumental variable sample weight simulation"
author: "Miao Cai"
date: "12/21/2018"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: hide
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Instrumental variable

Consider a simple regression model:

$$Y = \beta_0 + \beta_1 Tr + \epsilon$$

An instrumental variable $IV$ is correlated with $Tr$ but uncorrelated with the outcome$Y$except through $X$.

# An overview

Suppose that you have a continuous variable \( y \) with the known mean
response function

\[ E(y) = \beta_0 + \beta_1 x + \beta_2 c \]

and further that $x$ and $c$ are correlated with each other. If you knew $c$,
estimating $\beta_1$ would be easy by simply running `lm(y ~ x + c)`. You
would complete the regression, throw together a few diagnostics and call it a
day.

But often we don't observe $c$ in our data. $c$ could be any number of things,
such as treatment practices at a hospital or unmeasured differences between
patients, but it is in the direct casual path of $y$ and you don't know it. If
you instead do the regression `lm(y ~ x)`, you will not get an accurate
estimate of $\beta_1$. If you think about the model fit in that call it looks
like

\[ y_i = \beta_0 + \beta_1 x_i + \epsilon_i \]
but in this case
\[ \epsilon_i = f(c_i, \eta_i) \]

where $\eta_i$ is white noise centered on zero. As long as that function is
linear, the expectation of $\epsilon$ will be some multiple of $c$. The result
is that $x$ and $\epsilon_i$ are correlated. The estimates of $\beta_1$ will
nessecarly be wrong. Interested parties can read more [here](https://en.wikipedia.org/wiki/Instrumental_variables_estimation#Estimation)
and see just how wrong it will be.

Suppose there is a third variable $z$, $x$ can be expressed as
some function of $z$, $z$ has no effect on $y$ except through $x$ (therefore $z$ 
is independent of any other variable that effects $y$). Then we have the
equations


\[ E(x) = \alpha_0 + \alpha_1 x^* + \alpha_2 z \]
and
\[ E(y) = \beta_0 + \beta_1 x + f(c) \]

where $x^*$ is some latent part of $x$ and $c$ is still unobserved. Looking at
the first equation and realizing we don't know what $x^*$ is the resulting model
will have two regression coefficients. The intercept will be
$\alpha_0 + \alpha_1 E(x^*)$ which is not correlated with $f(c)$ as
and $z$ is independent of $f(c)$ by assumption.

If we take the fitted values of $x$ from the first equation and plug it into the
second equation, the $x$ term is no independent of $f(c)$. This independence
means that we can produce consistent estimates of $\beta_1$ when we replace $x$ 
with the fitted values of $x$ from equation one. When the estimator used is OLS,
this method is called two-stage least squares (2SLS).

```{r dat}
library(MASS)
# we are really generating x* and c and using a common variance
xStarAndC <- mvrnorm(1000, c(20, 15), matrix(c(1, 0.5, 0.5, 1), 2, 2))
xStar <- xStarAndC[, 1]
c <- xStarAndC[, 2]
z <- rnorm(1000)
x <- xStar + z
y <- 1 + x + c + rnorm(1000, 0, 0.5)
```



[R blogger reference](https://www.r-bloggers.com/instrumental-variables-simulation/)


# One endogeneous variable

## A simple demonstration of bias

```{r demoone}
library(MASS)
library(tidyverse)
nobs = 1000

r1 = 0.3
b1 = 1
b2 = 2

mt = matrix(c(1, r1, r1, 1),
            nrow = 2, ncol = 2)

dat = data.frame(mvrnorm(n=1000, mu=rep(2,2), Sigma = mt))
dat = mutate(dat, Y = b1*X1 + b2*X2 + rnorm(nobs, 0, 1) )

lmfitT = lm(Y ~ X1 + X2, data = dat)
(coeffT = summary(lmfitT)$coefficients[,1])

lmfitF = lm(Y ~ X1, data = dat)
(coeffF = summary(lmfitF)$coefficients[,1])

coeffT[2] - coeffF[2]

ggplot(dat, aes(x = X1, y = Y, color = X2)) + geom_point() + 
  geom_abline(intercept = coeffT[1], slope = coeffT[2], color = "green") + 
  geom_abline(intercept = coeffF[1], slope = coeffF[2], color = "red")
```


## increase sample size

```{r one}
##### function ######
onefunction = function(nobs = 1000, r1 = 0.3, b1 = 1, b2 = 1){
  mt = matrix(c(1, r1, r1, 1),
            nrow = 2, ncol = 2)

  dat = data.frame(mvrnorm(n=nobs, mu=rep(2,2), Sigma = mt))
  dat = mutate(dat, Y = b1*X1 + b2*X2 + rnorm(nobs, 0, 1) )
  
  lmfitT = lm(Y ~ X1 + X2, data = dat)
  coeffT = summary(lmfitT)$coefficients[,1]
  lmfitF = lm(Y ~ X1, data = dat)
  coeffF = summary(lmfitF)$coefficients[,1]
  
  return(coeffF[2])
}

# increase the number of obs
bias = vector(length = 100L)
for (i in (1:100)*1000){
  bias[i/1000] = abs(onefunction(nobs = i) - 1)
}

data.frame(n = (1:100)*1000, bias = bias) %>% 
  ggplot(aes(x = n, y = bias)) + geom_line() + 
  ggtitle("Increase sample size")
```

## Increase correlation coefficient

```{r increaseR}
# increase the number of obs
bias = vector(length = 1000L)
for (i in (0:999)/1000){
  bias[i*1000 + 1] = abs(onefunction(r1 = i) - 1)
}

data.frame(r = (0:999)/1000, bias = bias) %>% 
  ggplot(aes(x = r, y = bias)) + geom_line() +
  ggtitle("Increase correlation coefficient")
```


```{r increaseb1}
bias = vector(length = 100L)
for (i in 1:100){
  bias[i] = abs(onefunction(b1 = i, b2 = 1) - i)
}

data.frame(b1 = 1:100, bias = bias) %>% 
  ggplot(aes(x = b1, y = bias)) + geom_line() +
  ggtitle("Increase b1")
```

```{r increaseb2}
# increase the number of obs
bias = vector(length = 100L)
for (i in 1:100){
  bias[i] = abs(onefunction(b2 = i) - 1)
}

data.frame(b2 = 1:100, bias = bias) %>% 
  ggplot(aes(x = b2, y = bias)) + geom_line() +
  ggtitle("Increase b2")
```


# Two confounders

```{r two}
library(MASS)
nobs = 1000

r1 = 0.3
r2 = 0.5
r3 = 0.8

mt = matrix(c(1, 0.3, 0.5, 0.8, 
              0.3, 1, 0, 0,
              0.5, 0, 1, 0,
              0.8, 0, 0, 1),
            nrow =3, ncol = 3)

x1 = rnorm(nobs, 2, 2)


```




# Three confounders

```{r three}
library(MASS)
nobs = 1000

r1 = 0.3
r2 = 0.5
r3 = 0.8

mt = matrix(c(1, 0.3, 0.5, 0.8, 
              0.3, 1, 0, 0,
              0.5, 0, 1, 0,
              0.8, 0, 0, 1),
            nrow =3, ncol = 3)

x1 = rnorm(nobs, 2, 2)


```

