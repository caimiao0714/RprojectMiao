---
title: "Scalable Data Processing in R"
author: "Miao Cai"
date: "1/9/2019"
output:
  html_document:
    theme: yeti
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    code_folding: hide
  pdf_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Why my code is slow?

- complexity of calculations
- Carefully consider disk operations to write fast, scalable code

Benchmarking read and write performance in R

```{r benchmark, warning=FALSE}
library(microbenchmark)
set.seed(666)
summary(microbenchmark( t0 = rnorm(100), t1 = rnorm(10000) ))
```

## How does processing time vary by data size?

If you are processing all elements of two data sets, and one data set is bigger, then the bigger data set will take longer to process. However, it's important to realize that how much longer it takes is not always directly proportional to how much bigger it is. That is, if you have two data sets and one is two times the size of the other, it is not guaranteed that the larger one will take twice as long to process. It could take 1.5 times longer or even four times longer. It depends on which operations are used to process the data set.

```{r}
# Load the microbenchmark package
library(microbenchmark)

# Compare the timings for sorting different sizes of vector
mb <- microbenchmark(
  "1e5" = sort(rnorm(1e5)), # Sort a random normal vector length 1e5
  "2.5e5" = sort(rnorm(2.5e5)), # Sort a random normal vector length 2.5e5
  "5e5" = sort(rnorm(5e5)), # Sort a random normal vector length 5e5
  "7.5e5" = sort(rnorm(7.5e5)),
  "1e6" = sort(rnorm(1e6)),
  times = 10
)

# Plot the resulting benchmark object
summary(mb)
```

