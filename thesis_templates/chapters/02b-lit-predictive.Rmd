Predictive models
-----------------
### Overview

There are two cultures in current statistical or data science field, explanation and prediction [@shmueli2010explain; @breiman2001statistical]. The pro-explanation culture has long been adopted by most desciplines, such as epidemiology, economics, and psychology. In these desciplines, researchers commonly use generalized linear models, such as logistic regression and Poisson regression, to explain the association between the outcome and predictor variables. In contrast, the pro-prediction culture has recently been adopted in data science disciplines, in which they use blackbox algorithms such as random forests, decision trees, and neural networks to achieve similarly high prediction accuracy in training and testing sets. Pro-explanation models tend to excel at explaining the association between predictors and the outcome variable and being less likely to overfit the data. However, compared with machine learning and deep learning algorithms, pro-explanation models are less likely to capture potential interaction between predictor variables since they are conceptual framework driven. Therefore, pro-explanation models generally have less prediction accuracy compared with black-box algorithms.

Traffic safety field has a pro-explanation culture, although it is shifting towards a pro-prediction by adopting cutting-edge machine learning and deep learning algorithms. The most commonly used statistical models for are logistic regression and Poisson regression. Logistic regression is commonly used to predict crash likelihood (probability) using real-time data, for example traffic and weather at 5-minute intervals [@wang2017safety]. In contrast, Poisson regression is used to predict the crash frequency (the number of crashes) within a time period using aggregated data such as average data traffic and precipitation. I will briefly introduce the two models and then compare the two cultures of predictive models in statistical and machine learning perspective.

The parameterization of a binary logistic regression is shown in Model \@ref(eq:logit).
\begin{equation}
\begin{split}
Y_i & \sim \text{Bernoulli}(p_i) \\
\log(\frac{p_i}{1 - p_i}) & = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
(\#eq:logit)
\end{split}
\end{equation}

Where $Y_i$ is a binary variable that indicates whether an event occurred or not in the $i$-th observation. $p_i$ is the mean parameter of a Bernoulli distribution, which is constrained on $[0, 1]$. The logit transformation of $p_i$ then has the range from $-\infty$ to $+\infty$, which equals a linear combination of the predictors $x_1, x_2, \cdots, x_k$ and associated parameters $\beta_0, \beta_1, \cdots, \beta_k$.

The most commonly used outcomes for binary logistic regressions are injury versus non-injury crashes or fatal versus non-fatal crashes [@savolainen2011statistical]. For example, @chen2016driver used a two-level hierarchical Bayesian logistic model to predict the likelihood of high-severity crashes compared to low-severity crashes in New Mexico, accounting for both crash-level and driver-level effects. They found that road curve, functional and disabled vehicle damage, single-vehicle crashes, female, older drivers, drug or alcohol involvement were associated with increased odds of severe crashes. Considering the rare-event natural of crashes, @theofilatos2016predicting used logistic regression with rare events bias correction and Firth method to study significant risk factors for crashes in Greece. They found a negative association between crash likelihood and speed in crash locations. The proportion of trucks on the road was included in their model but not found to be significant. Other traffic safety studies using logistic regressions include but were not limit to @moudon2011risk, @meuleners2017determinants, @ahmed2018effects. There are two excellent systematic reviews on traffic crash likelihood predictions by @roshandel2015impact and @xu2015calibration.

Other variants of a binary logistic regression are binary probit models [@lee2008presence; @yu2014using], ordered logistic or probit models [@xie2009crash; @zhu2011comprehensive], multinomial logit models [@ye2011investigation]. There are only minor difference between a probit model and a logistic model. A logistic model uses the inverse logit of the linear predictors to calculate the probability of an event, as shown in Equation \@ref(eq:inverselogit); a probit model uses the cumulative normal density function of the linear predictors to calculate the probability, as shown in Equation \@ref(eq:probit). The error function $\text{erf}(x)$ is an integral without an analytical solution: $\text{erf}(x) = \frac{2}{\pi}\int_0^xe^{-t^2}dt$.
\begin{equation}
p = \text{logit}^{-1}(\mathbf{X}^\prime\beta) = \frac{\exp (\mathbf{X}^\prime\beta)}{1 + \exp (\mathbf{X}^\prime\beta)}
(\#eq:inverselogit)
\end{equation}
\begin{equation}
p = \Phi(\mathbf{X}^\prime\beta) = \frac{1}{2}\Big[1+\text{erf}(\frac{\mathbf{X}^\prime\beta}{\sqrt 2})\Big]
(\#eq:probit)
\end{equation}

Ordered logistic or probit regressions aim to model an ordered multi-category outcome variable. The most common case is study the severity of crashes, such as no-injury crashes, minor-injury crashes, and fatal-injury crashes [@zhu2011comprehensive]. These ordered models account for the ranked nature of different severity levels but make the proportional odds assumption [@rifaat2012severity]. When the proportional odds assumption is violated, researchers often switch to multinomial logit or probit models, in which the outcome variable is deemed as nominal.

On the other hand, the parameterization of a Poisson regression is shown in model \@ref(eq:poisson).
\begin{equation}
\begin{split}
Y_i^\star & \sim \text{Poisson}(\mu_i) \\
\log\mu_i & = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_kx_k
(\#eq:poisson)
\end{split}
\end{equation}

Where $Y_i^\star$ is the number of events in the $i$-th observation, which must be a non-negative integer. $\mu_i$ is the mean and variance of the Poisson distribution, and it must be a non-negative numeric value. The logarithm of $\mu_i$ transforms $\mu_i$ into the range of $(-\infty, +\infty)$, which equals a linear combination of the predictors $x_1, x_2, \cdots, x_k$ and associated parameters $\beta_0, \beta_1, \cdots, \beta_k$. Note that the mean parameter equals the variance parameter in the Poisson distribution, which is often violated in real-life data. When the variance of the data is greater than expected, it is called overdispersion. Otherwise, it is called underdispersion. Overdispersion is much more common than underdispersion in statistical practice.

When researchers have crash data that are aggregated over a long time period such as one year, it often makes sense to study the number of crashes instead of whether a crash occurred or not since they are often more than one crash. The most commonly used statistical model is therefore Poisson model, as it well handles count data that are right-skewed, long tailed, and only have non-negative integer values. For example, @cantor2010driver used Poisson regressions to explore the association between the rate of crashes driver-level characteristics among 560,695 commercial truck drivers in the United States. They found that past safety performance, out-of-service rate, body mass index (BMI), age, and the number of unique companies were strong predictors of the rate of truck crashes. 
Other variants of a Poisson model includes negative binomial models, quasi-Poisson models, and zero-inflated Poisson or negative binomial models [@lord2006modeling; @mohammadi2014crash]. Negative binomial or quasi-Poisson models are developed to account for the overdispersion and underdispersion in count data, for which a Poisson model fails to account. Zero-inflated Poisson or negative binomial models are developed to account for the feature of rare events in traffic crash data [@lord2005poisson; @lord2007further; @washington2010statistical; @dong2014multivariate]. There is an excellent review paper on statistical models for crash frequency data by @lord2010statistical.


Recently, recurrent event models have also been applied to model the change in intensity of SCEs in the traffic safety field. For example, @liu2019assessing proposed to use a mixed-effects Poisson process (a recurrent-event model) to model unintentional lane deviation events, with the baseline intensity and time-varying coefficients modeled by penalized B-splines. The first conducted a simulation study to assess the performance of the proposed model with different curvature of time-varying coefficients and the magnitude of event rate. Simulated 500 data sets with 500 shifts per set suggested satisfactory estimates for the true Gamma fragility parameter $\phi$ as estimated by an expectationâ€“maximization algorithm, where larger values of $\phi$ indicated greater heterogeneity between shifts and more intense events. The bias $\phi$ in the simulation ranged from $-0.01$ to $-0.09$, which was around 2% smaller and 0.6% smaller than the true value in low and high event rate settings respectively. They  applied the proposed model to 96 commercial truck drivers including 1,880 shifts. The study found that shifts with normal sleep time (7-9 hours) had a lower intensity compared with insufficient (< 7 hours) and abundant ($\geq 9$ hours) sleep time shifts.

<!-- 

- Change point analysis: @li2017evaluation; @li2018bayesian
- Frailty framework: @chen2016evaluating

-->

### Bayesian models

In contrast to traditional frequentist models that view parameters as unknown but fixed values, Bayesian models view parameters as random variables that have probability distributions [@gelman2013bayesian]. Researchers have subjective prior beliefs (a probability distribution) on these parameters $p(\theta)$ before they collect any data. After observing the data $\mathbf{X}$, the researchers could change their prior beliefs. Therefore, the posterior distribution $p(\theta | \mathbf{X})$ is an an unconditional distribution that is a compromise between the prior beliefs and the data. This compromise is given analytically by the Bayes Theorem (Equation \@ref(eq:bayes)).
\begin{equation}
\begin{split}
p(\theta | \mathbf{X}) & = \frac{p(\theta)p(\mathbf{X}|\theta)}{p(\mathbf{X})} \\
 & = \frac{p(\theta)p(\mathbf{X}|\theta )}{\int p(\theta)p(\mathbf{X}|\theta)d\theta}
(\#eq:bayes)
\end{split}
\end{equation}

Where $p(\mathbf{X}|\theta)$ is the likelihood function, which reflects the data generating process that gives rise to the data observed. The denominator $\int p(\theta)p(\mathbf{X}|\theta)d\theta$ is a normalizing constant that makes the posterior distribution integrates to one. The prior and likelihood function are straightforward since they both have analytical forms. The trickiest part of Bayesian inference is the normalizing constant in the denominator [@gelman2013bayesian; @kruschke2014doing]. 

The normalizing constant need to make the posterior distribution integrate to one since the posterior is supposed to be a probability density distribution. When there are more than two parameters in the model, the normalizing constant often becomes intractable since it involves integration in multiple dimensions. Modern Bayesian inference often uses numerical methods such as Markov chain Monte Carlo (MCMC) methods to directly sample from this posterior distribution, or the integrated Laplace approximation to approximate this constant. However, this numerical methods often fail or take an inhibitively long time to solve the problem with the presence of high-dimensional data or very tall data in this era.

There are several strengths of Bayesian models over traditional Frequentist models. First, the probabilistic distribution of parameters, posterior credibles intervals, and posterior predictive distributions account for the uncertainty in parameters and the data generating process. They also have straightforward and intuitive interpretations. Second, Bayesian models incorporate prior information $p(\theta)$ into the statistical model, which can be useful when there is sufficient prior background information. This prior distribution (regularizing priors) is particularly useful for estimation in high-dimensional, sparse data settings, and complex statistical models such as hierarchical models [@betancourt2015hamiltonian; @mcelreath2018statistical]. Lastly, Bayesian models are scalable to complex data generating process. This is because modern Bayesian estimation is powered by numerical methods and simulation, which in essence only requires researchers to specify the priors and likelihood function. The difficulty of written the likelihood function is minimal compared to traditional Frequentist approaches such as the maximum likelihood estimation, which scales with the complexity of models [@lambert2018student].


### Hierarchical models

Most studies on traffic safety assume that the sampling unit is a spatial-temporal segment, which is a specific section of a road with relatively high rate of crashes during a period. However, it is not sufficient to only study the occasions where the crashes are more likely to occur; we must also study the non-crashes and compare them with crashes. On the other hand, these studies that focus on road segments ignore driver-level unobserved effects. It is reported that the chance of having crashes for truck drivers with crash history in the past year is nearly twice as high as those without crash history in the past year [@cantor2010driver]. Most motor carrier insurance companies and employers also view historical safety events as an important measure of the driver's performance. Therefore, it is more natural to use driver-focused models to account for unobserved variation and characteristics associated with vehicle drivers [@huang2010multilevel].

In the Bayesian perspective, a hierarchical model is a statistical model with the probability distribution of one parameter depends on another parameter [@kruschke2015bayesian]. Suppose we have a model with two parameters $\alpha, \beta$ and data $D$. The joint prior distribution of the two parameters is $p(\alpha, \beta)$. According to the Bayes Theorem, the posterior distribution is proportional to the product of the prior distribution and the likelihood function: $P(\alpha, \beta|D) \propto P(\alpha, \beta)P(D|\alpha,\beta)$. In a hierarchical model setting, the product can be factored as a chain of products among parameters, also known as conditional independence, such as $P(\alpha, \beta)P(D|\alpha,\beta) = P(D|\beta)P(\beta|\alpha)P(\alpha)$. In this parameterization, the parameter $\alpha$ is known as the hyperparameter because it gives rise to the parameter $\beta$ (the parameter of a parameter) [@kruschke2015bayesian].

Model \@ref(eq:hierarchicalbayes) demonstrates a random-intercept hierarchical logitistic regression that predicts the likelihood of safety events. The outcome $Y_{i, d(i)}$ is a binary variable that indicates whether a safety event occurred or not, and it has a Bernoulli distribution with the mean parameter $p_{i, d(i)}$. The logit transformation of $p_{i, d(i)}$ can then be predicted by $k$ variables $x_1, x_2, \cdots, x_k$. The random intercept $\beta_{0, d(i)}$ determines that this is hierarchical model since they vary across different drivers $d(i)$. This model assumes that these random intercepts are sampled from a population of drivers with the mean of $\mu_0$ and standard deviation of $\sigma_0$, which are known as hyperparameters.
\begin{equation}
\begin{split}
Y_{i, d(i)} &\sim \text{Bernoulli}(p_{i, d(i)})\\
\log\frac{p_{i, d(i)}}{1-p_{i, d(i)}} &= \beta_{0, d(i)} + \beta_1x_{1,i} + \beta_2x_{2,i} + \cdots + \beta_kx_{k,i}\\
\beta_{0, d(i)} &\sim N(\mu_0, \sigma_0^2), \quad k = 1, 2, \cdots, D
(\#eq:hierarchicalbayes)
\end{split}
\end{equation}
Compared with traditional fixed-effects models that either pool all groups of data or estimate separate models individually for each group, a hierarchical model has the advantage of partial pooling across different groups [@mcelreath2018statistical]. This partial pooling shrinks group-level parameter estimates towards the group mean and shares information across groups. Therefore, with reasonable assumptions on the data generating process, estimates from a hierarchical model are generally more robust to extreme observations and reasonably accurate for those groups with sparse data [@gelman2006data; @lambert2018student].

Hierarchical models also come with costs. They are particularly known for its complexity to estimate to coefficients in both Frequetist maximum likelihood and Bayesian estimation. The de facto way of current Bayesian estimation is Markov chain Monte Carlo (MCMC). However, in the hierarchical model setting, it is difficult for MCMC to efficiently sample from the posterior distributions of hyperparameters due to the correlation between different levels of parameters, as well as the large number of parameters created by the hierarchical structure.


### Markov chain Monte Carlo (MCMC)

In modern statistics, Bayesian inference almost indispensably relies on Markov chain Monte Carlo (MCMC) sampling to overcome the intractable denominator in the Bayes Theroem (Equation \@ref(eq:bayes)). A **Monte Carlo simulation** is a technique to understand a target distribution by generating a large amount of random values from that distribution [@kruschke2014doing]. A **Markov chain** has the property that the probability distribution of the observation $i$ only depends on the previous observation $i-1$, not on any one prior to observation $i-1$, as demonstrated in Equation \@ref(eq:markovchain). 
\begin{equation}
P\left(X_{i}=x_{i} | X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{i-1}=x_{i-1}\right) = P\left(X_{i}=x_{i} | X_{i-1}=x_{i-1}\right)
(\#eq:markovchain)
\end{equation}
Integrating Markov chains and Monte Carlo simulations, the MCMC method can characterize an unknown unconditional distribution without knowing its all mathematical properties by sampling from the distribution [@van2018simple]. It has been widely applied in multiple fields such as statistics, physics, chemistry, and computer science [@craiu2014bayesian].  The most notable application of MCMC is probably in Bayesian inference, in which it has been used to draw samples from the posterior distribution and calculate relevant statistics (such as mean, standard deviation, and intervals). 

The first proposal of using MCMC dates to the paper by @metropolis1953equation, in which they tried to solve an intractable integral with a random walk MCMC. The Metropolis algorithm starts with a randomly defined initial value of the parameter $\theta$. From a pre-defined symmetric proposal probability distribution  $p(\theta | \mathbf{x})$, it then draw a proposal parameter value $\theta^{(\text{prop})}$, which only depends on the current parameter value $\theta^{(t)}$. This proposal value will be accepted with the probability of $\alpha$ defined in Equation \@ref(eq:metropolisalpha).
\begin{equation}
\alpha = \min\bigg(1, \frac{p(\theta^{(\text{prop})}|\mathbf{x})}{p(\theta^{(t)}|\mathbf{x})}\bigg)
(\#eq:metropolisalpha)
\end{equation}

This proposal and acceptance with probability steps will be iterated for a pre-define number of times. When the Metropolis algorithm reaches a steady state, these proposal values are random values drawn from the posterior distribution of parameter $\theta$, which can be used to describe and characterize the posterior distribution. 

After decades of successful empirical trials in physics, @hastings1970monte proposed a more generalized form of the Metropolis algorithm, in which the proposal distribution can be arbitrary, but the acceptance probability $\alpha^\star$ is modified as shown in Equation \@ref(eq:MHalpha). This Metropolis-Hasting (MH) algorithm is the most classic and widely-known MCMC algorithm used in multiple fields.

Let $p(\mathbf{\theta|X})$ be the posterior distribution we want to know, then the *Metropolis-Hasting algorithm* is:
  
1. Let $\theta^{(1)}$ denote an initial value for the continuous state Markov chain,
2. Set $t = 1$,
3. Let $q$ be the proposal density which can depend on the current state $\theta^{(t)}$. Simulate one observation $\theta^{(\text{prop})}$ from $q(\theta^{(\text{prop})} | \theta^{(t)})$,
4. Compute the following probability:
\begin{equation}
\alpha^\star = \min\bigg(1, \frac{p\left(\theta^{(\text{prop})} | \boldsymbol{x}\right)}{p\left(\theta^{(t)} | \boldsymbol{x}\right)} \frac{q\left(\theta^{(t)} | \theta^{(\text{prop})}\right)}{q\left(\theta^{(\text{prop})} | \theta^{(t)}\right)}\bigg)
(\#eq:MHalpha)
\end{equation}
5. Set $\theta^{(t+1)} = \theta^{(\text{prop})}$ with the probability of $\alpha^\star$; otherwise set $\theta^{(t+1)} = \theta^{(t)}$. Set $t \leftarrow t + 1$ and return to 3 until the desired number of iterations is reached.

Although the M-H algorithm is simple and powerful for performing MCMC, its performance highly depends on the choice of the proposal distribution. When there are a few parameters in the model and the proposal distribution is not well-designed, the M-H algorithm will have a very low acceptance rate, which makes the M-H algorithm very inefficient. In view of this issue, Gibbs sampler was proposed with the idea that the proposed values are always accepted and each parameter is updated one at a time by generating samples from the conditional distributions [@geman1987stochastic; @gelfand1990sampling; @lambert2018student]. The development of the software *Bayesian inference Using Gibbs Sampler (BUGS)* [@lunn2000winbugs; @lunn2009bugs] was critical in increasing the popularity of applied Bayesian analyses considering its support for a wide variety of statistical distributions, automatic application of the Gibbs Sampler, and numerous textbooks, tutorials and discussion.

Suppose $\mathbf{\theta} = [\theta_1, \theta_2, \cdots, \theta_k]$ is a $k$-dimensional parameter. Let $\mathbf{X}$ denote the data. The *Gibss sampling* algorithm is then:
  
1. Begin with an estimate $\mathbf{\theta}^{(0)} = [\theta_1^{(0)}, \theta_2^{(0)}, \cdots , \theta_k^{(0)}]$ in the parameter space,
2. Set $t = 1$,
3. Simulate $\theta_1^{(t)}$ from $p(\theta_1|\theta_2^{(t-1)}, \theta_3^{(t-1)},\cdots , \theta_k^{(t-1)}, \mathbf{X})$,
4. Simulate $\theta_2^{(t)}$ from $p(\theta_2|\theta_1^{(t)}, \theta_3^{(t-1)},\cdots , \theta_k^{(t-1)}, \mathbf{X})$,
5. $\cdots$,
6. Simulate $\theta_k^{(t)}$ from $p(\theta_k|\theta_1^{(t)}, \theta_3^{(t)},\cdots , \theta_{k-1}^{(t)}, \mathbf{X})$,
7. Set $t \leftarrow t + 1$ and repeat steps $3-6$ for a pre-specified number of iterations and make sure the Gibbs sampler reaches the steady state for a sufficient number of iterations.

The generality of the M-H algorithm and Gibbs sampler and the simplicity in software packages in `R` or `BUGS` help them gain popularity among applied researchers in the recent 30 years. However, as more and more data are available in applied field, the performance of the two most popular MCMC methods has been widely criticized [@betancourt2019convergence]. The performance of the M-H algorithm crucially depends on the proposal distribution. An efficient proposal distribution in M-H algorithm should generate random draws with less auto-correlation, which enables more effective exploration of the parameter space [@quiroz2015bayesian]. On the other hand, the performance of the Gibss Sampler crucially depends on the parameter structure. If there is a significant correlation between parameter estimates, the Gibbs Sampler will become very inefficient as the geometry of the distribution is not aligned with the stepping directions of each sampler [@lambert2018student].


Scalable Bayesian models
------------------------

Recent ten years witnessed an explosive growth of data size and dimensionality.  This poses a major challenge to Bayesian methods using MCMC. Traditional MCMC algorithm need to evaluate the entire data at each step of iteration, which could be expensive for computation in the case of tall data [@bardenet2017markov]. In applied analysis, researchers often need to set  thousands of iterations to reach stable posterior distribution, which takes hours or days to implement a single model. Besides, when the researchers have high dimensional data where high-probability regions are concentrated on a extremely limited region of sample space, it would very hard for random-walk MCMC to generate samples from these small regions [@barp2018geometry]. Hierarchical models even complicate this issue by adding random parameters for each subgroup, which further grows the dimensionality of parameter space. Furthermore, when there is high correlation between different parameters that often occur in the case of many parameters, neither the M-H algorithm or Gibbs sampler can efficiently generate samples from the posterior distribtion. All the aforementioned problems motivate researchers in different fields to develop different scalable algorithms to make Bayesian inference for big data.

### Hamiltonian Monte Carlo (HMC)

The M-H algorithm and Gibbs sampler can be very inefficient in big data settings because of sparse high-density parameter space, high costs of evaluating the entire data at each step, or a high correlation between parameters. Originally proposed by @duane1987hybrid with the name of Hybrid Monte Carlo, the Hamiltonian Monte Carlo (HMC) modifies the random-walk behavior in M-H algorithm into a deterministic one by adding auxiliary momentum parameters $p_n$, thus more efficiently explores the high-density regions in big data settings compared to the traditional M-H algorithm or the Gibbs sampler [@betancourt2017conceptual; @wang2019hamiltonian]. Although HMC was originally proposed in 1987 [@duane1987hybrid], it is only widely adopted by applied researchers in the recent five years, thanks to the development of the No-U-Turn Sampler (NUTS) [@hoffman2014no] and the statistical programming language `Stan` [@carpenter2017stan].

Let $\boldsymbol{q}$ denote the position vector and $\boldsymbol{p}$ denote the momentum vector in the conservative dynamics physics system. Note that $\boldsymbol{q}$ and $\boldsymbol{q}$ must have the same length. The combination $(\boldsymbol{q}, \boldsymbol{p})$ then defines a position-momentum phase space, which can be calculated using the conditional distribution [@neal2011mcmc; @betancourt2017conceptual]:
$$\pi(\boldsymbol{p}, \boldsymbol{q}) = \pi(\boldsymbol{p}|\boldsymbol{q})\pi(\boldsymbol{q})$$

This joint distribution can also be defined in terms of the *Hamiltonian*:
$$\pi(\boldsymbol{p}, \boldsymbol{q}) = e^{-H(\boldsymbol{p}, \boldsymbol{q})}$$
After a little bit of transformation, we have:
\begin{equation}
\begin{aligned}
H(\boldsymbol{p}, \boldsymbol{q}) &= -\log \pi(\boldsymbol{p}, \boldsymbol{q})\\
&= -\log\pi(\boldsymbol{p}|\boldsymbol{q}) - \log\pi(\boldsymbol{q})\\
&= K(\boldsymbol{p}, \boldsymbol{q}) + V(\boldsymbol{q})
(\#eq:hamiltonian)
\end{aligned}
\end{equation}

In the perspective of physics, the *Hamiltonian* $H(\boldsymbol{p}, \boldsymbol{q})$ is the total energy of the system, which composes of two parts: *kinetic energy* $K(\boldsymbol{p}, \boldsymbol{q})$ and *potential energy* $V(\boldsymbol{q})$. Note that the potential energy $V(\boldsymbol{q}) = -\log\pi(\boldsymbol{q})$ is essentially the negative log of the posterior distribution of the parameter posterior density $\boldsymbol{q}$.

In a static system, the Hamiltonian is a constant. The evolution of this system is governed by the *Hamiltonian equations*:
\begin{equation}
\begin{aligned}
\frac{d \boldsymbol{q}}{dt} &= \frac{\partial H}{\boldsymbol{p}} = \frac{\partial K}{\boldsymbol{p}}\\
\frac{d \boldsymbol{p}}{dt} &= -\frac{\partial H}{\boldsymbol{q}} = -\frac{\partial K}{\boldsymbol{q}} - \frac{\partial V}{\boldsymbol{q}}
(\#eq:hamiltonianequation)
\end{aligned}
\end{equation}

It turns out that we can randomly generate high density proposals in the parameters space by taking advantage of the Hamiltonian system. Here is the general idea if the *HMC algorithm* [@lambert2018student]: 

1. Let $\theta^{(0)}$ denote a random initial value from a proposal distribution,
2. Set $t = 1$,
3. Generate a random initial momentum $m$ from a proposal distribution (typically a multivariate normal distribution),
4. Use the leapfrog algorithm to solve the trajectory moving over the high-density posterior parameter space under the Hamiltonian mechanism for a period,
5. Calculate the new momentum $m^\prime$ and new position $\theta^{(\text{prop})}$
6. Compute the following probability:
\begin{equation}
\alpha^H = \min\bigg(1, \frac{p\left(\theta^{(\text{prop})} | \boldsymbol{x}\right)p(\theta^{(\text{prop})})}{p\left(\theta^{(t)} | \boldsymbol{x}\right)p(\theta^{(t)})} \frac{q(m^\prime)}{q(m)}\bigg)
(\#eq:hmc)
\end{equation}
7. Set $\theta^{(t+1)} = \theta^{(\text{prop})}$ with the probability of $\alpha^H$; otherwise set $\theta^{(t+1)} = \theta^{(t)}$. Set $t \leftarrow t + 1$ and return to 3 until the desired number of iterations is reached.

The HMC is essentially an improved form of M-H algorithm by using the Hamiltonian to generate distant and effective proposals instead of naive random-walk and revised form of the acceptance probability (Equation \@ref(eq:hmc)).

Two parameters need to be tuned when implementing the HMC: step size $\epsilon$ and the optimal trajectory length $T$. The optimal trajectory length is the product of the number of steps $L$ and step size $\epsilon$ [@neal2011mcmc; @monnahan2017faster]. The step size $\epsilon$ decides how similarly the sympletic methods (typically the leapfrog algorithm) imitates the true unnormalized posterior density. If $\epsilon$ is too small, it will take a lot of steps for the leapfrog algorithm to explore the posterior space. If $\epsilon$ is too big, the leapfrog algorithm will loop around and return to a place near its original step. The trajectory length $T = \epsilon L$, which need to be tuned in similar style with $\epsilon$: if $L$ is too short, it will be hard to simulate distant proposal and the algorithm is inefficient; if $L$ is too long, the trajectory will loop back and become computationally inefficient. Hand tuning these two parameters was the major obstacle to implement HMC for applied researchers.

The No-U-Turn Sampler (NUTS) proposed by @hoffman2014no solves the difficulty of hand tuning $\epsilon$ and $T$ in static HMC. NUTS calculates the optimal step size $\epsilon$ and number of steps $L$ through a tree building algorithm [@monnahan2017faster]. The tree depth $k$ is defined as the number of doublings, resulting in $2^k$ leapfrog steps to build the trajectory. This $k$ is then decided by repeating the doubling iterations until the trajectory 'makes a U-turn' (loops back) or diverges (the Hamiltonian expands to infinity). Therefore, the NUTS can automatically create trajectories that can efficiently explore the high-density parameter space without having to hand tune $\epsilon$ and $T$.


### Subsampling MCMC

With rapid development of automatically data collection system, more tall and wide data are becoming commonly available to researchers. A tall dataset has many observations or rows, while a wide dataset has many variables or columns. The emergence of big data poses a threat to the existing MCMC algorithms, as most of them require that the full data likelihood be evaluated at each iteration, which will be computationally intensive in the case of tall and wide data. One way to tackle the computational burden of evaluating the full data likelihood is subsampling MCMC, which means evaluating the likelihood based on a subset of data. Subsampling MCMC via simple random sample often does not work as it does not account for the variability of the log likelihood estimator among different subsamples. The most popular technique of performing subsampling MCMC is via introducing auxiliary variables that reduce the variability of log likelihood estimators [@quiroz2018subsampling].

The first well-known subsampling MCMC algorithm is the firefly MCMC by @maclaurin2015firefly, which introduces an auxiliary variable for each observation that can be turned on or off to determine if the observation should be included in likelihood evaluation. Starting from this firefly MCMC algorithm, an increasing number of studies have been published on subsampling MCMC algorithms. @korattikara2014austerity proposed to use a sequential hypothesis test to generate *accept-reject samples* with high confidence on a fraction of data. Similar studies that use accept-reject samples include @bardenet2014towards and @bardenet2017markov. Another category of widely discussed subsampling MCMC algorithm is Pseudo-Marginal MCMC (PMCMC), which replaces the likelihood or the natural logarithm of likelihood with an unbiased estimate from a subset of data based on control variates at each MCMC iteration [@quiroz2018subsampling; @quiroz2019speeding]. They proposed two types of bias-correction log-likelihood estimates: a) parameter expanded control variates via Taylor expansion around a reference value in parameter space, and b) data expanded control variate via Taylor expansion around the nearest centroid in data space. Other subsampling MCMC algorithms include Block-Poisson estimator [@quiroz2016block], delayed acceptance [@quiroz2018speeding], noisy MCMC [@alquier2016noisy], and zig-zag process MCMC [@bierkens2019zig].

Apart from the subsampling MCMC algorithms being mentioned, subsampling MCMC using the Hamiltonian mechanism deserves special attention as it efficiently explores the posterior in high-dimensional parameter space. However, HMC is especially inefficiently in the case of tall data as the gradient will be very expensive. @chen2014stochastic proposed a stochastic gradient HMC, which introduces a friction term that counteracts the effects of noisy gradient. In contrast, @betancourt2015fundamental argued that the stochastic gradient HMC proposed by @chen2014stochastic compromised the scalability of the HMC with respect to the complexity of the target distribution. The paper claimed that subsampled data does not have sufficient information to efficiently explore the target distributions, and devastates the scalable performance of HMC. A recent paper by @dang2019hamiltonian extended the PMCMC algorithm by @quiroz2019speeding to HMC via introducing a fictitious momentum vector $\vec{p}$, which has the same dimension as the parameter vector $\mathbf{\theta}$.

<!--
[Stochastic Gradient HMC](https://blog.csdn.net/u013841458/article/details/82495450)
-->



